---
title: "NHANES Data Cleaning"
author: "Kevin S.W. --- UNI: ksw2137"
date: "`r format(Sys.time(), '%x')`"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, 
                      fig.dim = c(10, 4), 
                      fig.align = "center",
                      results = "asis"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8))
          )

```

# Data Processing

Now that we've explored the variables, found out what each of them represent, and selected for several variables of interest, we are ready to go to the next step. That is to find any kind of correlation between the variables. This step is important because we want our analysis to be worthwhile, thus requiring that there is at least some correlation between our variables of interest.

# Merging Activity Data and Flag Data

First, we need to re-arrange our activity data and make it less cumbersome. The current plan is to aggregate all of the activity counts into "daily counts", thus making it easier to parse through.

```{r dataload}

library(rnhanesdata)

# turning original dataset into long format
activity_data_D <- PAXINTEN_D %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("MIN"), 
    names_to = "min",
    names_prefix = "min",
    values_to = "activ_count") %>% 
  mutate(
    min = as.numeric(min)
    ) %>% 
  mutate_at(
    .vars = vars("seqn", "paxcal", "paxstat", "weekday", "sddsrvyr"),
    .funs = funs(factor)
    ) %>% 
  group_by(seqn)


flag_data_D <- Flags_D %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("MIN"), 
    names_to = "min",
    names_prefix = "min",
    values_to = "activ_count") %>% 
  mutate(
    min = as.numeric(min)
    ) %>% 
  mutate_at(
    .vars = vars("seqn", "paxcal", "paxstat", "weekday", "sddsrvyr"),
    .funs = funs(factor)
    ) %>% 
  group_by(seqn)


activity_flag_joined <- 
  left_join(
    activity_data_D, 
    flag_data_D, 
    by = c("seqn", "paxcal", "paxstat", "weekday", "sddsrvyr", "min")) 

```

# Filtering Dataset

Once the dataset has been merged, we now proceed to filter out the dataset based on values that are reliable. In this particular case, we filter so that `activity_flag`, `paxstat`, and `paxcal` = 1. This is because 1 indicates that the tracking device are worn, reliable, and calibrated.

```{r}

filtered_activity_flag_joined <- activity_flag_joined %>% 
  rename(activity_count = activ_count.x,
         activity_flag = activ_count.y) %>% 
  filter(activity_flag == 1,
         paxcal == 1,
         paxstat == 1)

total_activity_df <- filtered_activity_flag_joined %>% 
  group_by(seqn) %>% 
  summarize(total_count = sum(activity_count))

#total_activity_df %>% write_csv("./Datasets/total_activ_data.csv")

total_activity_df %>% 
  ggplot(aes(x = total_count)) +
  geom_histogram() + 
  theme(legend.position = "none")

```

Once we filter it out, we can create a dataset out of this. For now, the total activity per subject will suffice to explore some general associations. If we need to obtain a more granular dataset, we could do so with the datasets prior to filtering. Additionally, we could plot the distribution of total activities. We see that the data is highly right-skewed. 

Below, we also would like to evaluate the average count of daily activity. This is obtained by first obtaining the daily average and then averaging it out further. While we're at it, we will also evaluate the distribution of both of these "averages"

```{r}

avg_activity <- filtered_activity_flag_joined %>% 
  group_by(seqn, weekday) %>% 
  summarize(avg_daily = mean(activity_count))

#avg_activity %>% write_csv("./Datasets/avg_daily_activ_data.csv")

avg_activity %>% 
  ggplot(aes(x = avg_daily, color = weekday, fill = weekday)) +
  geom_histogram() + 
  theme(legend.position = "none")




grand_avg_activity <- avg_activity %>% 
  ungroup() %>% 
  group_by(seqn) %>% 
  summarize(grand_avg = mean(avg_daily))

#grand_avg_activity %>% write_csv("./Datasets/grand_avg_activ_data.csv")

grand_avg_activity %>% 
  ggplot(aes(x = grand_avg)) +
  geom_histogram() + 
  theme(legend.position = "none")

```

