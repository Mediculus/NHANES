---
title: "Directed EDA for rnhanes Package"
author: "Kevin S.W. --- UNI: ksw2137"
date: "`r format(Sys.time(), '%x')`"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE, message = FALSE,
                      fig.dim = c(10, 4), 
                      fig.align = "center")

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr", "httr", "rvest")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8))
          )

```

# Filtering Covariate Data of Interest

Since we've explored almost all possible datasets that can be found in the `rnhanesdata` package, we can now make a more targeted approach to which dataset we want to work on as well as obtaining particular raw data information. First though, we need to load the package as always.

```{r package_load}

library(rnhanesdata)

```

## Picking which NHANES Data

The original package has 2 waves, C and D, that corresponds to surveys sent out between 2003-2004 and 2005-2006, respectively. Working with wave D has advantage because not only is the data more recent, it also has more "raw" observations and thus potentially more datapoints we could work with. 

```{r raw_covar_load, results = "hide"}

covar_data_d <- as_tibble(
  process_covar(
    waves = "D", extractAll = TRUE
    )[[1]] # process_covar outputs a list of 1 containing the tibble, this extracts the "content" and 
  ) %>%    # turn it into tibble
  janitor::clean_names()

covar_names <- names(covar_data_d)

```

We can then filter out these observations to only include those that may be useful for accelerometer and flag data (in case we ever want to explore relationships between these variables).

First, we load up the flag data, `Flags_D` to obtain the unique ID of each participants (`seqn`). The reason why we used flag data is because we only wanted participants who have information on their wearable status (whether it is "reliable" or not at a given minute/time).

```{r seqn_obtain}

identifier <- Flags_D %>% 
  janitor::clean_names() %>% 
  distinct(seqn) %>% 
  mutate(
    seqn = as.integer(seqn)
  )

covar_data_d <- left_join(identifier, covar_data_d, by = 'seqn')

```

## Filtering our Covariates Data

In another document, we've succesfully created a .csv file that contained all the variables of interest with respect to variables that might be associated with cardiovascular health. We can use that .csv file to filter our `covar_data_d`, which will then allow us to proceed in analyzing the data. 

```{r covar_filtering}

variable_list <- read_csv("./Datasets/variable_list.csv")

covar_d_clean <- select_if(covar_data_d, 
                           names(covar_data_d) %in% pull(variable_list, var_name))

```

## Exploring Data Frame

We would like to explore our clean data now and firstly check the amount of `NA` in each column. 

```{r NA_check}

na_check_df <- covar_d_clean %>% 
  map_df(~sum(is.na(.))) %>% 
  pivot_longer(2:103,
               names_to = "var_name",
               values_to = "na_count") %>% 
  arrange(desc(na_count)) 

```

Upon arranging the resulting `NA` count into a descending order, something peculiar showed, which is that the variables `padtimes`, `padactiv`, and `padlevel` from the package appear to have more `NA` compared to the number of `seqn` (respondents). Investigating it further leads to the finding that this particular variable is saved as a vector variable, containing many more observations. Thankfully, this is likely to be redundant variable when we consider including our activity level data from the package. Thus decision was made to exclude these.

```{r remove_pad_vars}

na_check_df <- na_check_df %>% 
  filter(!(var_name %in% c("padtimes", "padactiv", "padlevel")))

```


```{r filter_NA, echo = FALSE, eval = FALSE}

#We then evaluate from the remaining `r nrow(na_check_df)` variables and felt that an `NA` amount that is >`r round(100*((7455-1000)/7455), 3)`% would not be a good representative of the variable. Therefore, we filtered all those that have values greater than `r 7455*(7455-1000)/7455`. 

#na_check_df <- na_check_df %>% 
#  filter( na_count < 6455)

```

## Re-selecting Viable Variables for Data Analysis 

Now that we've set a level of acceptable responses, we could revisit our variables and be more selective.

```{r}

clean_variable_list <- left_join(na_check_df, variable_list, by = "var_name")

```

Revising the variables, these are the ones we decide to NOT include:

* mcq010: asthma dx
* mcq025: age of 1st asthma
* mcq035: still have asthma currently
* mcq300a: close relative had MI?
* mcq300b: close relative had asthma?
* mcq300c: close relative had diabetes?
* bmx***: 
  * sub: subscapular skinfold (mm)
  * armc: arm circumference
  * calf: calf circumference
  * thicr: thigh circumference
  * tri: triceps skinfold
* bpxchr: Heart rate measurements for 0-7 yrs old
* bpxpty: extra variable
* dmdfmsiz: total # of people in the family
* wtint2yr: total weight over 2 years follow-up based on report
* wtmec2yr: total weight over 2 years follow-up based on exam
* did040: age first told by clinician that subject had diabetes
* diq180: had blood tested in past 3 years
* did060: how long on insulin
* diq060u: units for `did060`
* did070: taking diabetic medications
* diq230
* diq240
* did250
* did260
* diq260u
* did270
* diq280
* diq290
* diq300
* did310
* did320
* did330
* did340
* did350
* diq350u
* diq360
* diq080
* mcq025
* mcq300a
* mcq300b
* mcq300c

```{r 2nd_var_remove}

filter_remove_list <- 
  c("mcq010", "mcq025", "mcq035", "mcq300a", "mcq300b", "mcq300c", 
    "bmxsub", "bmxarmc", "bmxcalf", "bmxthicr", "bmxtri", "bmxsub",
    "bpxchr", "bpxpty", "wtint2yr", "wtmec2yr", "did040", "diq180", "did060",
    "diq060u", "did070", "diq230", "diq240", "did250", "did260", "diq260u",
    "did270", "diq280", "diq290", "diq300", "did310", "did320", "did330", 
    "did340", "did350", "diq350u", "diq360", "diq080", "mcq025", "mcq300a",
    "mcq300b", "mcq300c", "pfq030", "smq020", "smd030", 
    "dmdfmsiz")

clean_variable_list <- clean_variable_list %>% 
  filter(!(str_detect(var_name, 
                      paste(filter_remove_list,            
                            collapse = "|"))))

```

We then further re-clean our `covar_d_clean` dataframe

```{r recleaned_covar}

covar_d_clean <- select_if(covar_data_d, 
                           names(covar_data_d) %in% c("seqn", pull(clean_variable_list, var_name)))

```

## Further Dimension Reduction

Evaluating the covariates further, we can now begin to properly assess whether we could average, total, or modify these covariates of interest to whittle down at the sheer number of it. 

For one, we have `bpxsy` and `bpxdi` variables, which are blood pressure measurements over 4 days. We could average these out and create and `avg_bp` variable.

### Averaging Blood Pressure Measures

Since we have 4 measures of blood pressure systolic/diastolic, we decided to average this to reduce the amount of covariates. There are some data that are missing and these will be "ignored" (i.e. average out of available data).

```{r}

processed_covar_d_clean <- covar_d_clean %>% 
  mutate(
    bpxsy_avg = round(rowMeans(.[, c("bpxsy1", "bpxsy2", "bpxsy3", "bpxsy4")], na.rm = TRUE), 3),
    bpxdi_avg = round(rowMeans(.[, c("bpxdi1", "bpxdi2", "bpxdi3", "bpxdi4")], na.rm = TRUE), 3)
  ) %>% 
  select(seqn, sddsrvyr, ridstatr, riagendr, ridageyr, ridreth1, everything(), -bpxsy1, -bpxsy2, -bpxsy3, -bpxsy4, -bpxdi1, -bpxdi2, -bpxdi3, -bpxdi4)



processed_covar_d_clean %>% 
  drop_na(bpxsy_avg) %>% 
  filter(diq010 %in% c(1, 2, 3)) %>% 
  mutate(
    diq010 = factor(diq010)
  ) %>% 
  ggplot(aes(color = diq010, fill = diq010)) +
    geom_density(aes(x = bpxsy_avg), alpha = 0.5)

processed_covar_d_clean %>% 
  drop_na(bpxdi_avg) %>% 
  filter(diq010 %in% c(1, 2, 3)) %>% 
  mutate(
    diq010 = factor(diq010)
  ) %>% 
  ggplot(aes(color = diq010, fill = diq010)) +
    geom_density(aes(x = bpxdi_avg), alpha = 0.5)

```

## Final Touches to Covariate Dataset

We now check for any distributions, min-max and so on using `skimr`

```{r skim_data}

skimr::skim(processed_covar_d_clean)

```

Once we check distributions and satisfied with results, we should also make this into another dataset so that we don't have to keep re-running the scripts. Do note that this is just the very first iteration of what our covariates of interest are.

```{r}

#processed_covar_d_clean %>% write_csv("./Datasets/covariates.csv")

```



