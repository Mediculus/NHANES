---
output: 
  html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, fig.dim = c(10, 4), fig.align = "center", results = "asis")


```

<br></br>

<center><a href=""><img src="img/nhanes_apple_color_tagline.png"></a></center>

<br></br>

## About This Project

Hi! This project was initially an attempt to explore the `rnhanesdata` package and its contents. Due to some unexpected development however, this project developed into building a predictive model for my disease of interest--diabetes--using resources from the `rnhanesdata` package. 

Over the development of this project, I faced various challenges and issues that became an opportunity to learn more about survey data, machine-learning, imbalanced class, sub-sampling methods, and metrics associated with these approaches. A proper report of this project can be found [here](). This page however, will give you a brief overview of what I learned throughout this whole endeavor. It will be divided into several major sections:

* `rnhanesdata` Package
* NHANES Survey Dataset
* Machine Learning Using `tidymodels`
* Imbalanced Class Problem and How to Mitigate Them

## `rnhanesdata` Package

The package housed 2 major datasets from NHANES that was associated with activity measures. They were the 2003-2004 and 2005-2006 survey waves (labeled C and D respectively). There are 6 major datasets in total; 2 for activity measures, 2 for activity flags, and 2 for covariates, corresponding to each waves. 

The activity measures dataset contained records of daily minute-by-minute activity measures (intensity) for the duration of 1 week. It appears that these activities are recorded through some wearable device. Similarly, the activity flags recorded daily minute-by-minute activity measures but these are binary measures of 0/1 indicating whether or not the wearable device was actually worn. The function `exclude_accel()` from the package allows us to compare the two datasets and keep only the activity datapoints which were deemed "reliable". 

The other datasets contained an already-processed covariate dataset. Some of the variables include `BMI`, `Race`, `Gender`, or `DrinkStatus`. In its original state, there are 16 variables included in the covariate datasets, not including survey-design-related variables. However, the package does provide a function to extract more covariates, if needed, using the function `process_covar()`. The function also has an option to `extractAll = TRUE` if we decided to just get all possible covariates but this is defaulted to `FALSE`.

More details can be read [here](./rnhanesdata_eda/rnhanesdata_explore.html)

## NHANES Survey Dataset

Since this was also my first time being exposed to any kind of survey-dataset, NHANES was an incredibly overwhelming source for me. After I attempted to extract all covariates using the `rnhanesdata::process_covar()` function, I was surprised to see that there's 300+ variables from NHANES. This led me to build a scraping function that pulls the variable names and their descriptions. 

In the process, I learned that NHANES divided their variables into demographic, dietary, exam, laboratory, questionnaire, and "limited" data. Ultimately though, by collecting these variables' descriptions into a dataframe, it really helped me get a much better sense of what NHANES recorded and quickly select which variables I'd like to focus more on. Additionally, I was also able to quickly assess which variables might be named differently but measured the same thing. 

More can be read [here](./codebook_scrape/codebook_data.Rmd)

## Machine Learning using `tidymodels`

As the project developed into building predictive models, I had to re-orient myself with all the tools available in R. My supervisor advised me that LASSO and/or Random Forest are likely good starting points for a predictive model. At first, I attempted to use `caret` but this proved quite confusing as the function language was something that I wasn't used to. My supervisor recommended me to use the actual packages instead (`glmnet` and `ranger`). 

While I eventually managed to use all these packages, I also came across `tidymodels`, the apparent successor to `caret` and appeared to play extremely well with the `tidyverse` package. With a copious amount of Google-ing, watching YouTube videos, and/or reading blogs that showcases its use, I became much more comfortable with `tidymodels` and ended up using this package to do my modeling instead. 

You can check my [report]() to see how I've combined both `tidyverse` and `tidymodels` techniques to answer my project questions!

## Imbalanced Class Problem and How to Mitigate Them

During my learning about predictive modeling, I came across an issue that, even until now, puzzles me. Up to this point, my supervisor and I couldn't figure out the cause behind it, except that my outcome being "imbalanced" and/or the variables I selected were weakly predictive for diabetes. What happened was my early models had incredibly poor predictive performance. It boasted a 99% accuracy but when you assess its prediction, it actually has 100% specificity and 0% sensitivity. 

As I dug deeper on what may cause the issue, I found that this issue--imbalanced class--has been a quite well-known problem with machine learning. It occurs when (assuming binary outcome) a class has an uneven distribution. In my case, those with diabetes accounts for ~15% of my dataset. Further research on how to mitigate this issue led me to sub-sampling methods. Examples include random over-sampling/under-sampling, SMOTE, or ADASYN. Part of this project is to assess the improvement gained after applying these sub-sampling methods. 

Additionally, I also learned that the conventional metrics such as accuracy or even ROC-AUC may not adequately detect changes/improvements in the model because some of these metrics may not fully account for the class imbalance. Some articles suggest that we use other metrics such as precision, recall, F-measure, J-index, G-mean, or precision-recall-AUC. As such, in this project, we also tried to compare different metrics and see which one could detect changes in a model's performance given this imbalanced class problem. 

Ultimately, we found that G-mean was one of the metrics that was able to reasonably detect changes in performance given the imbalance. Furthermore, certain sub-sampling methods did perform worse but overall, they improved the default model. Nevertheless, I have quite a strong suspicion that part of the reason for my default model's poor performance was because my dataset had variables that were not strongly predictive of diabetes, further worsened by the imbalance in the dataset. Next step after this project would likely explore modeling using boosted models such as XGBoost. 

You can check my [report]() to see how I utilized sub-sampling methods, obtained these alternate metrics, and evaluated them to arrive at my final model.

