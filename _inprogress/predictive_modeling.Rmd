---
title: "Predictive Modeling"
author: "Kevin S.W. --- UNI: ksw2137"
date: "`r format(Sys.time(), '%x')`"
output: 
  html_document:
    highlight: pygments
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.dim = c(10, 4), 
                      fig.align = "center"
                      )


# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "patchwork", "ghibli", "arsenal", #"kableExtra",
              "survey", "tidymodels", "themis", "vip")
invisible(lapply(Packages, library, character.only = TRUE))


# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0, size = 12),
                  plot.subtitle = element_text(hjust = 0, size = 8))
          )

```

<div style="text-align: right">[Return to Homepage](./index.html)</div>
<br></br>

This section details the process of cleaning up the generated NHANES dataset, exploratory analyses, model tuning, and model comparison to ultimately select the best-performing model. 

# Dataset pre-modeling exploration

## Load required packages

To do our initial work, we need these packages.

```{r load_package1, eval = FALSE}
library(tidyverse)    # general data wrangling
library(patchwork)    # easy way to combine various ggplot objects neatly
library(ghibli)       # some extra color palettes beyond viridis
library(arsenal)      # summary table creations
# library(kableExtra)   # extra features for kable package
```

We then load our dataset obtained that was generated from the process described [here](./data_generation.html)

```{r dataset_load}
# load dataset from .rda file that DOESN'T contain NA
load(file = "./_data/nhanes_data.rda")
```

Recall that our inclusion/exclusion criteria are:

* Those aged 20+ years old.
* Non-pregnant female (including those who had experienced menopause)

## Variable removal

The dataset is cleaned further to remove variables that are unnecessary for our particular model-building. Firstly, we notice that `paxcal`, `paxstat`, and `weekday` are no longer necessary. We also removed `sddsrvyr` because we've obtained re-adjusted 4-year survey weight metrics (if needed). `drinks_per_week` is also removed because we have a somewhat-correlated variable that are easier for classification, `drink_status`. Similarly, we will keep `bmi_cat` and remove `bmi` given that one is a categorical analogue of the other. Additionally, `na_any` as well as `na_sum` is no longer necessary.

```{r remove_vars}
# list of variables to be removed
rm_var <- c("paxcal", "paxstat", "weekday", "sddsrvyr","bmi", "drinks_per_week", "na_any", "na_sum")

# remove the variables
nhanes_df <- nhanes_df %>% select(-all_of(rm_var))
rm(list = "rm_var")
```

## Checking for factor levels and adjustment as necessary

We want to evaluate the factor levels and ensure that they are coded/ordered properly so that our regressions/modeling functions take the proper outcomes of interest, etc.

```{r check_factors}
nhanes_df %>% select_if(is.factor) %>% str()
```

Certain variables are slightly disordered and we should re-arrange them properly where it makes sense.

```{r factor_relevel}
# relevel stroke to have "No" as reference, and drink status to be "Non-Drinker" as the reference.
nhanes_df$stroke <- fct_relevel(nhanes_df$stroke, c("No", "Yes"))
nhanes_df$drink_status <- fct_relevel(nhanes_df$drink_status, c("Non-Drinker", "Moderate Drinker", "Heavy Drinker"))
```

## Table 1 Creation

```{r summary_table}
# table variable labels ---------
table_labels <- list(
  ridageyr = "Age (yrs)", 
  bmi_cat = "BMI Category (N %)", 
  race = "Race/Ethnicity (N %)",
  gender = "Gender (N %)",
  chf = "Diagnosis/History of CHF (N %)",
  chd = "Diagnosis/History of CHD (N %)", 
  cancer = "Diagnosis/History of Cancer (N %)", 
  stroke = "Diagnosis/History of Stroke/TIA", 
  education_adult = "Highest Education (N %)",
  mobility_problem = "Mobility Problem (N %)",
  drink_status = "Alcoholic Drinking Status (N %)",
  smoke_cigs = "Cigarette Smoking Status (N %)", 
  bmxwt = "Weight (kg)",
  bmxht = "Height (cm)", 
  bmxwaist = "Waist Circumference (cm)",
  bpxpls = "Heart Rate (bpm)",
  lbxgh = "HbA1c level (%)",
  bpxsy_avg = "Systolic Blood Pressure (mmHg)",
  bpxdi_avg = "Diastolic Blood Pressure (mmHg)",
  pir_cat = "Poverty-Income-Ratio Category (N %)",
  mean_tdac = "Average Total Daily Activity Measure",
  mean_tdlac = "Average Total Log-Daily Activity Measure",
  mean_sedtime = "Average Daily Sedentary Time (min)",
  mean_mviatime = "Average Daily Moderate-to-Vigorous Activity Time (min)"
  )

# summary table controls -------------
table_controls <- tableby.control(total = T, 
                                  test = F,
                                  # numeric.test = "kwt",
                                  # cat.test = "chisq",
                                  numeric.stats = c("meansd", "medianq1q3", "range"),
                                  cat.stats = c("countpct"),
                                  stats.labels = list(
                                    meansd = "Mean (SD)",
                                    medianq1q3 = "Median (Q1, Q3)",
                                    range = "Min - Max",
                                    countpct = "N (%)"))

# base tableby object ---------------
base_summ_table <- tableby(diabetes ~ ., 
                         data = nhanes_df %>% 
                           select(-all_of(c("seqn", "sdmvpsu", "sdmvstra", 
                                            "wtint4yr_adj_norm", "wtmec4yr_adj_norm"))),
                         control = table_controls)
            

# Prints out the summary table --------------
table1 <- summary(base_summ_table,
        title = "Table 1: Summary",
        labelTranslations = table_labels,
        text = T,
        digits = 2)


table1 %>% 
   kable("latex", booktabs = TRUE, linesep = "", digits = 3, 
         caption = "Summary Statistics of each Variable")
# %>% 
#    kable_styling(latex_options = c("HOLD_position"),
#                 font_size = 9,
#                 full_width = TRUE) %>% 
#    column_spec(1, width = "14cm")
```

# Survey GLM

## Evaluating univariate significance of variables

Before we move further into building our predictive models and selecting the best, we should look into which variables are likely to be significant predictors. We will be requiring the help of `survey` library given the survey weights.

```{r load_package2, eval = FALSE}
library(survey)
```

After loading the package, we shall create our "survey design" first.

```{r survey_glm}
# apply controls for our sur
svy_ctrl <- svydesign(id= ~sdmvpsu, strata = ~sdmvstra, weights = ~wtmec4yr_adj_norm, data = nhanes_df, nest = TRUE)
```

We will then build multiple glm functions that assesses individual variables' association to diabetes using survey-glm. To do this, we will create a new dataframe containing the variable names and apply the models using `map()` functions.

```{r univariate_glm}
# vector of variables that will NOT be included in the model
svy_excl_var <- c("seqn", "sdmvpsu", "sdmvstra", "wtint4yr_adj_norm", "wtmec4yr_adj_norm")

# create tibble with the variable names as column, then use purrr to map the svyglm() function
# then use broom's tidy() and glance() to obtain metrics.
# models were checked and was predicting the proper target outcome (diabetes = "Yes")
survey_df <- tibble(
  var_names = names(nhanes_df %>% select(-all_of(svy_excl_var), -diabetes)),
  # apply glm using survey weights
  svy_mod = map(var_names, 
              ~svyglm(as.formula(paste0("diabetes ~ ", .x)), 
                      design = svy_ctrl, 
                      family = quasibinomial())),
  # apply glm WITHOUT survey weights
  glm_mod = map(var_names, 
              ~glm(as.formula(paste0("diabetes ~ ", .x)), 
                      family = binomial(), data = nhanes_df)),
  # obtain results using tidy()
  svy = map(svy_mod, broom::tidy),
  glm = map(glm_mod, broom::tidy)
  ) 


# remove the now-unneeded model column and unnest the result/stat columns
# filter out the "intercept" variables from the "term" column since we're not interested in those.
# also create new column; binary indicator for p-value < 0.05 (TRUE) or not (FALSE)
survey_df <- survey_df %>% select(-svy_mod, -glm_mod) %>% unnest(cols = c(svy, glm), names_sep = "_") %>% 
  filter(svy_term != "(Intercept)", glm_term != "(Intercept)") %>% 
  mutate(svy_pval_sig = ifelse(svy_p.value < 0.05, T, F),
         glm_pval_sig = ifelse(glm_p.value < 0.05, T, F),
         glm_pval_higher = ifelse(glm_p.value > svy_p.value, T, F))


# assess beta estimates
survey_df %>% 
  select(var_names, svy_term, ends_with(c("estimate", "p.value", "higher", "pval_sig")))


# filter for variables with p-values that are different between the models
# found 4 variables that changed; race, drink status, cigarette status, pulse quality
survey_df %>% filter(svy_pval_sig != glm_pval_sig) %>% 
  select(var_names, svy_term, svy_p.value, glm_p.value, glm_pval_higher, svy_pval_sig, glm_pval_sig)
```

## Applying Backwards Selection using Survey GLM

```{r manual_backwards_glm}
# variable list that will be tested with the model (test_vars)
# also create another variable (exc_vars) that will be modified during model selection; this is initiated as the start
# further modified within the for loop
exc_vars <- test_vars  <- distinct(survey_df, var_names) %>% filter(!(var_names %in% c("drink_status", "gender"))) %>% pull(var_names)

# create empty vectors to contain variables/values of interest necessary for backwards selection
# -1 to remove "intercept model" where no predictors are fitted
epic_vec  <- model_vec <- rep(NA, length(test_vars)-1)

# for loop to do manual backward selection
for(i in 1:(length(test_vars)-1)){
    # stores the AIC values for models run within a given i value
     epic_ij <- rep(NA,length(exc_vars))

    # another for loop that tries all possible combinations given the "current" exc_vars variables
    for(k in 1:length(exc_vars)){
        # create predictors excluding "k"th variable
        form    <- paste0(c(exc_vars[-k]), collapse="+")
        # fit the predictors
        fit_tmp <- svyglm(as.formula(paste("diabetes ~", form)), design = svy_ctrl, family=quasibinomial())
        # store the AIC values for each k
        epic_ij[k] <- extractAIC(fit_tmp, k=4)[2]
        # remove the temporary vectors
        rm(list=c("fit_tmp","form"))
    }
    # obtain the k value where AIC is minimal from the k models at a given i
    k_cur         <- which(epic_ij == min(epic_ij))
    # store the model predictors at this k value
    model_vec[i]  <- paste0(c(exc_vars[-k_cur]), collapse="+")
    # update exc_vars to remove the variable at k (because this k-th model gives the minimal AIC)
    exc_vars      <- exc_vars[-k_cur]
    # store the min AIC value for the "min-AIC" model at a given i
    epic_vec[i]   <- epic_ij[k_cur]
    # remove the "temporary" variables, then repeat the process with i+1
    rm(list=c("k_cur", "k", "epic_ij"))
}

# obtain the overall minimal AIC value
min(epic_vec)

# extract the model with the min AIC
model_vec[which(epic_vec == min(epic_vec))]

svyglm(as.formula(paste("diabetes ~", model_vec[which(epic_vec == min(epic_vec))])), design = svy_ctrl, family=quasibinomial())

glm(as.formula(paste("diabetes ~", model_vec[which(epic_vec == min(epic_vec))])), family=quasibinomial())
```

```{r clean_workspace, include = FALSE}
# clean workspace
rm(list=c("exc_vars", "i", "test_vars", "survey_df", "clean_survey_df", "svy_excl_var", "svy_ctrl"))
```

# Model Building and Testing

**NOTE: We only showed code without running it from pre-processing to tuning models section because we saved the finished tuning result object beforehand. This was done to save time compiling for future uses. Otherwise, each compile will take ~9 hours**

## Pre-processing steps

### Adding custom metrics

Based on the article by [source], the traditional metrics for assessing classification MLs will likely break when the binary outcome is imbalanced. 

For our particular dataset, we do have quite an imbalance with diabetes diagnosis distribution as described below. 

```{r percent_distro_diabetes}
nhanes_df %>% count(diabetes) %>% mutate(pct = n/sum(n))
```

[source] provided a way to classify the degree of imbalance by ***. If we calculate ours, we are on the higher end of "moderate imbalance". 

Their article further proposed alternative metrics that would potentially be more sensitive to the changes between sensitivity and specificity. 

Thus, we determined to obtain these metrics:

* ROC-AUC
* Precision
* Recall
* F-measure
* G-mean
* UAR

We need to add some of our own functions using `yardstick` to accommodate G-mean and UAR since `yardstick` doesn't have these, despite their comprehensive list of metrics. The code is not shown, but the construction referred to `yardstick`'s vignette on custom-metric-building.

```{r custom_metrics}
library(rlang)    # needed to create dataframe-friendly version of custom metrics

# prelim function to work with class metrics
event_col <- function(xtab, event_level) {
  if (identical(event_level, "first")) {
    colnames(xtab)[[1]]
  } else {
    colnames(xtab)[[2]]
  }
}

# G-mean metric custom function
g_mean_vec <- function(truth, 
                        estimate, 
                        estimator = NULL, 
                        na_rm = TRUE, 
                        event_level = "first",
                        ...) {
  estimator <- finalize_estimator(truth, estimator)
  
  g_mean_impl <- function(truth, estimate) {
    # Create 
    xtab <- table(estimate, truth)
    col <- event_col(xtab, event_level)
    col2 <- setdiff(colnames(xtab), col)
    
    tp <- xtab[col, col]
    fn <- xtab[col2, col]
    tn <- xtab[col2, col2]
    fp <- xtab[col, col2]
    
    sens <- tp/(tp+fn)
    spec <- tn/(tn+fp)
    
    sqrt(sens*spec)
  }
  
  metric_vec_template(
    metric_impl = g_mean_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "factor",
    estimator = estimator,
    ...
  )
}

# df version
g_mean <- function(data, ...) {
  UseMethod("g_mean")
}

g_mean <- new_class_metric(g_mean, direction = "maximize")

g_mean.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  
  metric_summarizer(
    metric_nm = "g_mean",
    metric_fn = g_mean_vec,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate), 
    na_rm = na_rm,
    ...
  )
  
}


# UAR custom-metric
uar_vec <- function(truth, 
                    estimate, 
                    estimator = NULL, 
                    na_rm = TRUE, 
                    event_level = "first",
                    ...) {
  estimator <- finalize_estimator(truth, estimator)
  
  uar_impl <- function(truth, estimate) {
    # Create 
    xtab <- table(estimate, truth)
    col <- event_col(xtab, event_level)
    col2 <- setdiff(colnames(xtab), col)
    
    tp <- xtab[col, col]
    fn <- xtab[col2, col]
    tn <- xtab[col2, col2]
    fp <- xtab[col, col2]
    
    sens <- tp/(tp+fn)
    spec <- tn/(tn+fp)
    
    (sens+spec)/2
  }
  
  metric_vec_template(
    metric_impl = uar_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "factor",
    estimator = estimator,
    ...
  )
}

# df version
uar <- function(data, ...) {
  UseMethod("uar")
}

uar <- new_class_metric(uar, direction = "maximize")

uar.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  
  metric_summarizer(
    metric_nm = "uar",
    metric_fn = uar_vec,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate), 
    na_rm = na_rm,
    ...
  )
  
}
```

We will then create a list for these metrics that will eventually be piped into our tuning process.

```{r metric_setup}
# metrics variable
modmetrics <- metric_set(roc_auc, precision, recall, f_meas, g_mean, uar)
```

### Subsetting dataset into test/train (holdouts)

Moving on to the actual dataset preprocessing, first we prepare our "model" dataset without id or weight variables.

```{r model_df_prep1, eval = FALSE}
#names(nhanes_df)

ml_df <- nhanes_df %>% select(-seqn, -sdmvpsu, -sdmvstra, -wtint4yr_adj_norm, -wtmec4yr_adj_norm)
```

We then create train and test datasets which will be applied to each subsampling method. So first, we should create a dataframe for the subsamples. We also create a helper function to be able to apply the same splits each time (just adding a `set.seed()` function before the split each time).

The split will be done by `rsample::initial_split()` function with the default proportion of 3/4 as training set.

```{r mapping_initialSplit, eval = FALSE}
# create a column for resampling method 
model_df <- tibble(sample_type = c("none", rep("oversample", 3), "undersample"), 
                   resamp_method = c("none", "smote", "bsmote", "adasyn", "down"))

# helper function to ensure the splits are exactly the same (duplicates)
seed_initialSplit <- function(df) {
  set.seed(123)
  split <- df %>% initial_split(strata = "diabetes")
}

# apply helper function
model_df <- model_df %>% 
  mutate(
    dataset = list(ml_df),
    split_data = map(dataset, seed_initialSplit),
    train = map(split_data, training),
    test = map(split_data, testing)
  )

model_df$resamp_method
```

### V-Fold Cross Validation for each Dataset

We also need to build a helper function to apply the same folding for each of the resampling applied to the subsampling methods. A 5-fold CV will be applied.

```{r mapping_vfoldcv, eval = FALSE}
# helper function with 5-fold as default
seed_vfold <- function(df, v = 5) {
  set.seed(123)
  fold <- vfold_cv(df, v = v, strata = "diabetes")
}

# apply vfold_cv to each subsampling setting
model_df <- model_df %>% 
  mutate(folds = map(train, seed_vfold))
```

### Building a recipe function that will apply further pre-procesing steps 

We will then employ the `recipes` package to proceed with a workflow blueprint of sorts.

First we create the `recipe()` for our model (i.e. blueprint). We also load `themis` package which is not officially a part of `tidymodels` yet.

```{r recipe_functions, eval = FALSE}
# function for applying subsampling method and obtaining our basic recipe
recipe_fn <- function(train_df, subsample = NULL, method = NULL) {
  
  df_rec <- recipe(diabetes ~ ., data = train_df) %>% 
    # create dummy variables for factors
    step_dummy(all_nominal(), -all_outcomes()) %>% 
    # apply zero-variance pre-processing
    step_zv(all_numeric(), -all_outcomes()) %>%
    # center and scale all numeric (note that step_dummy creates binary numeric variables)
    step_normalize(all_numeric(), -all_outcomes())
    
  
  if (subsample == "oversample") {
    
    if (method == "up") {
      df_rec <- df_rec %>% 
        step_upsample(diabetes)
    } else if (method == "smote") {
      df_rec <- df_rec %>% 
        step_smote(diabetes)
    } else if (method == "bsmote") {
      df_rec <- df_rec %>% 
        step_bsmote(diabetes)
    } else if (method == "adasyn") {
      df_rec <- df_rec %>% 
        step_adasyn(diabetes)
    } else if (method == "rose") {
      df_rec <- df_rec %>% 
        step_rose(diabetes)
    }
    
  } else if (subsample == "undersample") {
    
    if (method == "down") {
      df_rec <- df_rec %>% 
        step_downsample(diabetes)
    } else if (method == "nearmiss") {
      df_rec <- df_rec %>% 
        step_nearmiss(diabetes)
    } else if (method == "tomek") {
      df_rec <- df_rec %>% 
        step_tomek(diabetes)
    }

  }
  
  return(df_rec)
  
}

# apply function
model_df <- model_df %>% 
  mutate(
    rec_base = pmap(list(train, sample_type, resamp_method), 
                    ~recipe_fn(train_df = ..1, subsample = ..2, method = ..3))
  )


# since we now have a dataframe with all the necessary elements, we can remove the functions and dataframes that
# are no longer needed.
rm(list=c("seed_initialSplit", "seed_vfold", "recipe_fn"))
```

## Model specifications

### LASSO specification

`recipes` package also allows us to pre-specify our model specifications. So here, we will specify the engine to be used, as well as the tuning grid.

```{r lasso_mod_setup, eval = FALSE}
# LASSO spec
# mixture is alpha; 1 = LASSO
spec_lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

# Grid for lambda
# will build a grid for lambda that contains 100 values using the max_entropy
# randomization. As such, we need to set the seed.
set.seed(123)
grid_lasso <- spec_lasso %>% 
  # obtain parameters needed to be tuned from the lasso_spec object
  parameters() %>% 
  # apply the max-entropy randomization method
  grid_max_entropy(size = 100)
```

### Random Forest specification

Similarly, we will also pre-specify the model and tuning grid required to perform our random forest.

```{r rf_mod_setup, eval = FALSE}
# RF spec
spec_rf <- rand_forest(mtry = tune(), trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")


# Grid for RF
set.seed(123)
grid_rf <- spec_rf %>% 
  # obtain parameters needed tuning
  parameters() %>% 
  # "finalize" the max number of nodes, etc since it is dependent on column variables
  finalize(select(ml_df, -diabetes)) %>% 
  # apply max-entropy
  grid_max_entropy(size = 100)
```

### Building the workflow object for tuning models

Now that we have all the "ingredients" ready, we will build a `workflow::workflow()` object that allows us to run these tuning models smoothly.

### Building functions for workflow

Firstly, we need to build some functions that allow building workflows into the `model_df`.

```{r workflow_functions, eval = FALSE}
# function to initiate workflow
# specification, grid, and metrics should be a global object within session
wf_fn <- function(recipe, specification) {
  
  # create workflow template
  wf <- workflow() %>% 
    add_recipe(recipe) %>% 
    add_model(specification)
    
  return(wf)
  
}

# apply workflow function to the model_df
model_df <- model_df %>% 
  mutate(
    wf_lasso = map(rec_base, ~wf_fn(recipe = .x, specification = spec_lasso)),
    wf_rf = map(rec_base, ~wf_fn(recipe = .x, specification = spec_rf))
  )
```

## Tuning models

We then start tuning the model using another helper function that allows us to map this onto our `model_df` dataframe.

```{r tune_models, eval = FALSE}

# function to tune models
tune_fn <- function(workflow, resamples, grid, metrics) {
  
  set.seed(123)
  model_results <- workflow %>% 
    tune_grid(resamples = resamples,
              grid = grid,
              metrics = metrics,
              control = control_grid(verbose = T,
                                     event_level = "second", 
                                     save_pred = T)
              )

  return(model_results)
  
}


model_df <- model_df %>% 
  mutate(
    tune_res_lasso = map2(wf_lasso, folds, 
                          ~tune_fn(workflow = .x, resamples = .y, 
                                   grid = grid_lasso, metrics = modmetrics)),
    tune_res_rf = map2(wf_rf, folds, 
                       ~tune_fn(workflow = .x, resamples = .y, 
                                grid = grid_rf, metrics = modmetrics))
  )
```

```{r load_finished_model_df, include = FALSE}
# load compressed version
model_df <- read_rds(file = "./_data/finished_model_dataframe_git.rds")
```

### Tuning results re-arrangement

Now that we've obtained our tuning results stored into our `model_df`, we should apply some small dataset re-arrangement to make it slightly tidier.

```{r pivot_long}
model_df_long <- model_df %>% 
  pivot_longer(cols = wf_lasso:tune_res_rf,
               # apply specification to split into several columns; 
               # .value will be arbitrary from names_pattern 
               # while algorithm will contain the grouped regex
               # from names_pattern
               names_to = c(".value", "algorithm"),
               # builds 2 columns named wf_ or tune_res_, 
               # and puts the values into it respectively.
               # Then takes whatever remaining into algorithm
               names_pattern = "(wf_|tune_res_)(.+)") %>% 
  rename("workflows" = wf_, "tune_results" = tune_res_) %>% 
  select(algorithm, resamp_method, everything(), -sample_type)

rm(list=c("model_df"))
```

## Assessing the resampling metrics

### Evaluating the metrics by graphs

Create helper functions to obtain summaries, etc.

```{r helper_fn}
get_metric_summary <- function(data, algo) {
  
  data %>% 
    filter(algorithm == algo) %>% 
    # get metrics and predictions into their own columns
    mutate(metrics_summary = map(tune_results, collect_metrics),
           predictions = map(tune_results, collect_predictions)) %>% 
    # select just necessary columns for a summary df
    select(resamp_method, metrics_summary) %>% 
    unnest(cols = c(metrics_summary))
  
}
```

#### LASSO findings

```{r lasso_results1}
lasso_summary <- get_metric_summary(model_df_long, "lasso")

lasso_metric_graph <- lasso_summary %>% 
  ggplot(aes(x = penalty, y = mean, color = resamp_method, fill = resamp_method)) +
  geom_point(alpha = 0.3, size = 0.7) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~.metric) +
  ylab("Area under the ROC/PR Curve") +
  scale_x_log10(labels = scales::label_number())
```

Here, we see an anomaly with precision and it appears that somehow the function calculates the wrong "positive event". As such, it is unlikely to be a reliable predictor in this case. Nevertheless, it appears to be separating the subsampling methods incredibly well with 5 clearly-visible lines. 

Both F-measure and ROC-AUC was **unable** to distinguish any changes in ability to predict the minority class, which makes both measures unreliable as well.

On the other hand, we could see that G-mean, UAR, and recall to be quite capable of detecting those changes. However, recall is another way of saying sensitivity and is calculated as $\frac{TP}{TP+FN}$ where TP = true positive and FN = false negative. As such, it doesn't take into account the other column that contains specificity. Conversely, G-mean and UAR takes into account both sensitivity and specificity. G-mean also was able to obtain a better separation. As such, we will use G-mean as our metric to optimize. 

From these graphs, we can see that ADASYN and downsampling behaves quite similarly with ADAYSN slightly edging out downsampling methods. 

#### Random Forest findings

```{r rf_results}
rf_summary <- get_metric_summary(model_df_long, "rf")

rf_metric_graph <- rf_summary %>% 
  ggplot(aes(x = mtry, y = mean, color = resamp_method, fill = resamp_method)) +
  geom_point(alpha = 0.3, size = 0.7) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~ .metric) +
  ylab("Area under the ROC/PR Curve")
```

```{r patch_graph}
lasso_metric_graph / rf_metric_graph

rm(list=c("get_metric_summary", "rf_summary", "rf_metric_graph", "lasso_summary", "lasso_metric_graph"))
```

### Comparing best models from each resampling method

```{r show_best_metrics, echo = false, eval = false}
# get average, std of best model; for internal use
model_df_long %>% 
  select(algorithm, resamp_method, tune_results) %>% 
  mutate(best_hyperparam = map(tune_results, show_best, metric = "g_mean", n = 1)) %>% 
  select(-tune_results) %>% 
  unnest(cols = c(best_hyperparam)) %>% 
  arrange(algorithm, desc(mean))
```


```{r best_model_compoare}
results <- model_df_long %>% 
  select(algorithm, resamp_method, tune_results) %>% 
  mutate(best_hyperparam = map(tune_results, select_best, metric = "g_mean")) %>% 
  select(-tune_results) %>% 
  unnest(cols = c(best_hyperparam))

raw_metrics <- model_df_long %>% 
  select(algorithm, resamp_method, tune_results) %>% 
  unnest(cols = c(tune_results)) %>% 
  select(algorithm, resamp_method, id, .metrics) %>% 
  unnest(cols = c(.metrics)) %>% 
  filter(.metric %in% c("g_mean", "uar"))


# Boxplot for G-mean
results %>% left_join(raw_metrics, by = c("algorithm", "resamp_method", ".config")) %>% 
  filter(.metric == "g_mean") %>% 
  # pre-graphing processing to change appearance/order, etc
  mutate(
    algorithm = fct_recode(algorithm, "LASSO" = "lasso", "Random Forest" = "rf"),
    resamp_method = fct_relevel(resamp_method, "none", "adasyn", "smote", "bsmote", "down"),
    resamp_method = fct_recode(resamp_method,
                               "None" = "none", "ADASYN" = "adasyn", 
                               "SMOTE" = "smote", "BSMOTE" = "bsmote",
                               "Undersample" = "down")) %>% 
  select(algorithm, resamp_method, id, .estimate) %>% 
  # graph boxplot
  ggplot(aes(x = resamp_method, y = .estimate, color = algorithm, fill = algorithm)) +
  geom_boxplot(alpha = 0.3) + 
  # label settings
  labs(title = "G-mean Metric Resample Performance",
       subtitle = "Using best hyperparameters",
       x = "Resampling Methods",
       y = "G-mean Value",
       color = "Algorithm", fill = "Algorithm") +
  theme(legend.position = "right")
```

Seen from graphs ***

### Assessing ROC Curves

We'll also assess how the ROC curves perform given the imbalanced set in this case.

```{r resample_rocauc}
# resample ROC-AUC
rs_pred <- model_df_long %>% 
  select(algorithm, resamp_method, tune_results) %>% 
  unnest(col = c(tune_results)) %>% 
  select(algorithm, resamp_method, id, .predictions) %>% 
  unnest(col = c(.predictions))

rs_pred %>% 
  unite("model", algorithm:resamp_method) %>% 
  group_by(model, id) %>% 
  roc_curve(diabetes, .pred_Yes, event_level = "second") %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = id)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  facet_wrap( ~ model)
```

```{r clean_env}
rm(list=c("rs_pred", "results", "raw_metrics", "model_df_long"))
```


# Finalizing model

Now that we have identified the best model from our training, we will finalize it and apply the model to our whole training set and will validate it using our test set. 

We then develop our specifications for the various model we will try

```{r finalize_workflow, eval = FALSE}
# helper function to apply finalize_workflow to each model
finalize_wf_fn <- function(wf, params) {
  
  final_wf <- wf %>% 
    finalize_workflow(params)
  
  return(final_wf)
  
}

# apply the function and obtain the "final" dataframe
final_model_df_long <- model_df_long %>% 
  mutate(
    best_hyperparams = map(tune_results, select_best, metric = "g_mean"),
    final_workflow = map2(workflows, best_hyperparams, 
                         ~finalize_wf_fn(wf = .x, params = .y))
  )
```

Once we finalized our workflow, it is then time to fit it to the entire training set and evaluate using the hold-out test set.

```{r final_fitting, eval = FALSE}
# helper function to fit the final datasets
final_fit <- function(wf, split_scheme, metrics) {
  
  final_model <- wf %>% 
    last_fit(split_scheme, metrics = metrics)
  
  return(final_model)
  
}


# apply the function
final_dataframe <- final_model_df_long %>% 
  mutate(
    final_fit = map2(final_workflow, split_data, 
                     ~final_fit(wf = .x, split_scheme = .y, metrics = modmetrics))
  )
```

```{r load_final_dataframe, include = FALSE}
# load compressed dataframe
final_dataframe <- read_rds(file = "./_data/final_dataframe_git.rds")
```

## Final model assessments

### Confusion matrix

```{r confmat_get}
# function to get conf_mat components for each model
confmat_summarizer <- function(data) {
  
  data %>% 
    tidy() %>% 
    separate(col = name,
           into = c("cell", "row", "col")) %>% 
    select(-cell) %>% 
    mutate(prediction = factor(case_when(row == 1 ~ "No",
                                         row == 2 ~ "Yes"), 
                               levels = c("No", "Yes")),
           truth = factor(case_when(col == 1 ~ "No",
                                    col == 2 ~ "Yes"), 
                          levels = c("Yes", "No"))
           ) %>% 
    select(prediction, truth, value)
  
}

# obtain confusion matrix for each algorithm-subsampling pair
conf_mat_df <- final_dataframe %>% 
  select(algorithm, resamp_method, final_fit) %>% 
  unnest(col = c(final_fit)) %>% 
  select(algorithm, resamp_method, .predictions) %>% 
  unnest(col = c(.predictions)) %>% 
  group_by(algorithm, resamp_method) %>% 
  conf_mat(diabetes, .pred_class) %>% 
  select(algorithm, resamp_method, everything()) %>% 
  arrange(algorithm, resamp_method) %>%
  mutate(
    confmat_sum = map(conf_mat, confmat_summarizer),
    algorithm = factor(algorithm),
    resamp_method = factor(resamp_method, levels = c("none", "adasyn", "smote", "bsmote", "down"))
  ) %>% 
  select(-conf_mat) %>% 
  unnest(cols = c(confmat_sum))


# graph confusion matrix
conf_mat_df %>% 
  # cosmetic adjustments
  mutate(
    algorithm = fct_recode(algorithm, "LASSO" = "lasso", "Random Forest" = "rf"),
    resamp_method = fct_recode(resamp_method,
                               "None" = "none", "ADASYN" = "adasyn", 
                               "SMOTE" = "smote", "BSMOTE" = "bsmote",
                               "Undersample" = "down")) %>% 
  rename_with(str_to_title) %>% 
  ggplot(aes(x = Truth, y = Prediction)) +
  # fill color based on value (count), line color "white"
  geom_tile(aes(fill = Value), color = "white") +
  # manually set color gradient for "Value"
  scale_fill_gradient(low = "#F4F9FF", high = "#013676") +
  # add a binary color code for text, set to > 1000
  geom_text(aes(label = Value, color = Value > 1000)) +
  # apply manual text color for <1000, and >1000
  scale_color_manual(guide = FALSE, values = c("black", "white")) +
  facet_grid(Algorithm ~ Resamp_method, switch = "x") +
  scale_x_discrete(position = "top") +
  # extend the legend length to fit the number markers
  theme(legend.key.width = unit(1, "cm"))
```

### Final metrics

We use the code below to get all the final metrics except ROC-AUC from the confusion matrix.

```{r finmetrics_get}
# obtain metrics from the confusion matrix manually
# due to some metrics taking the wrong "event level"; unable to resolve
metric_calc <- conf_mat_df %>%
  mutate(
    # the cell type in a 2x2 table
    cell_type = case_when(prediction == "Yes" & truth == "Yes" ~ "tp",
                          prediction == "Yes" & truth == "No" ~ "fp",
                          prediction == "No" & truth == "Yes" ~ "fn",
                          prediction == "No" & truth == "No" ~ "tn")
  ) %>% 
  select(-prediction, -truth) %>% 
  pivot_wider(names_from = cell_type,
              values_from = value) %>% 
  select(algorithm, resamp_method, tp, fp, fn, tn) %>% 
  mutate(
    # obtain metrics
    sens = tp/(tp+fn),
    spec = tn/(tn+fp),
    ppv = tp/(tp+fp),
    npv = tn/(tn+fn),
    g_mean = sqrt(sens*spec),
    uar = (sens+spec)/2,
    f_meas = 2*((sens*ppv)/(sens+ppv))
  ) %>% 
  arrange(algorithm, desc(g_mean))

metric_calc %>% 
  select(algorithm, resamp_method, g_mean, uar, f_meas, ppv, sens)
```

Code below is used to obtain the ROC-AUC exclusively. Originally was intended to obtain all the metrics, but some metrics had a faulty event-level specification.

```{r rocauc_metric_get, echo = FALSE, eval = FALSE}
# originally part of final_assessment chunk to obtain metrics from last-fit objects.
# "deprecated" because found that certain measures was wrongly calculated because event_level
# was not set to "second" (i.e. certain metrics are measuring true negatives).


# df containing metrics and its final estimate (based on test set)
metrics <- final_dataframe %>%
  select(algorithm, resamp_method, final_fit) %>%
  unnest(col = c(final_fit)) %>%
  select(algorithm, resamp_method, .metrics) %>%
  unnest(col = c(.metrics))

# results for g-mean metric
metrics %>%
  filter(.metric == "roc_auc") %>%
  arrange(desc(.estimate)) %>%
  select(algorithm, resamp_method, everything(), -.estimator, -.config) %>%
  arrange(algorithm, desc(.estimate))
```

Code below graphs the ROC-AUC faceted by algorithm-subsampling combinations.

```{r final_rocauc}
# final ROC-AUC
pred <- final_dataframe %>% 
  select(algorithm, resamp_method, final_fit) %>% 
  unnest(col = c(final_fit)) %>% 
  select(algorithm, resamp_method, .predictions) %>% 
  unnest(col = c(.predictions))

pred %>%
  mutate(
    algorithm = fct_recode(algorithm, "LASSO" = "lasso", "Random Forest" = "rf"),
    resamp_method = fct_recode(resamp_method,
                               "None" = "none", "ADASYN" = "adasyn", 
                               "SMOTE" = "smote", "BSMOTE" = "bsmote",
                               "Undersample" = "down")
  ) %>% 
  group_by(algorithm, resamp_method) %>% 
  roc_curve(diabetes, .pred_Yes, event_level = "second") %>% 
  rename_with(str_to_title) %>% 
  ggplot(aes(x = 1 - Specificity, y = Sensitivity, color = Resamp_method)) +
  geom_path(alpha = 0.5) +
  geom_abline(lty = 3) +
  coord_equal() +
  facet_wrap(~ Algorithm) +
  theme(legend.position = "right")

rm(list=c("conf_mat_df", "metric_calc", "metrics", "pred", "confmat_summarizer"))
```

### Variable importance

```{r vip_get}
vip_df <- final_dataframe %>% 
  select(-sample_type, -dataset, -split_data, -folds, -rec_base, -workflows, -tune_results) %>% 
  filter(algorithm == "lasso", resamp_method == "adasyn")

vip_wf <- vip_df$final_workflow[[1]]

vi_df <- vip_wf %>% 
  fit(vip_df$train[[1]]) %>% 
  pull_workflow_fit() %>% 
  vi(lambda = vip_df$best_hyperparams[[1]]$penalty)

vi_df %>% 
  slice_max(Importance, n = 10) %>% 
  mutate(
    Variable = case_when(Variable == "lbxgh" ~ "HbA1c",
                         Variable == "bmxwaist" ~ "Waist Circumference",
                         Variable == "mean_tdac" ~ "Avg. Daily Activity Count",
                         Variable == "ridageyr" ~ "Age (yrs)",
                         Variable == "bpxdi_avg" ~ "Avg. Diastolic BP",
                         Variable == "chf_Yes" ~ "History of CHF",
                         Variable == "stroke_Yes" ~ "History of Stroke",
                         Variable == "race_Other.Hispanic" ~ "Other Hispanic",
                         Variable == "race_Mexican.American" ~ "Mexican American",
                         Variable == "pir_cat_Below.poverty" ~ "Below Poverty"),
    Sign = case_when(Sign == "NEG" ~ "Negative",
                     Sign == "POS" ~ "Positive")
  ) %>% 
  ggplot(aes(x = Importance, y = reorder(Variable, Importance), color = Sign, fill = Sign)) +
  geom_col() +
  scale_color_ghibli_d("KikiMedium", direction = -1) +
  scale_fill_ghibli_d("KikiMedium", direction = -1) +
  labs(title = "Variable Importance Plot",
       subtitle = "For LASSO-ADASYN, the best-performing overall model",
       y = "Variables") +
  theme(legend.position = "right")
  

rm(list=c("vip_df", "vip_wf", "vi_df"))
```


### Coefficient for LASSO

Since LASSO-ADASYN was the best model we obtained, we could also try to obtain the coefficients. 

```{r obtain_coefficients}
# helper function for pulling coefficients
coef_fn <- function(algorithm, wf, train_df) {
  
  if (algorithm == "lasso") {
    
    coef <- wf %>% 
      fit(train_df) %>% 
      pull_workflow_fit() %>% 
      tidy()
    
    return(coef)
    
  } 

}

# obtain coefficients
coefficients <- final_dataframe %>% 
  filter(algorithm == "lasso", resamp_method == "adasyn") %>% 
  mutate(
    coefs = pmap(list(algorithm, final_workflow, train), 
                 ~coef_fn(algorithm = ..1, wf = ..2, train_df = ..3))
  )


coefficients$coefs[[1]] %>% 
  filter(estimate != 0) %>% 
  select(-penalty) %>% 
  arrange(desc(estimate)) %>% 
  mutate(
    term = case_when(term == "lbxgh" ~ "HbA1c",
                     term == "bmxwaist" ~ "Waist Circumference",
                     term == "mean_tdac" ~ "Avg. Daily Activity Count",
                     term == "ridageyr" ~ "Age (yrs)",
                     term == "bpxdi_avg" ~ "Avg. Diastolic BP",
                     term == "chf_Yes" ~ "History of CHF",
                     term == "stroke_Yes" ~ "History of Stroke",
                     term == "race_Other.Hispanic" ~ "Other Hispanic",
                     term == "race_Mexican.American" ~ "Mexican American",
                     term == "pir_cat_Below.poverty" ~ "Below Poverty",
                     term == "drink_status_Moderate.Drinker" ~ "Moderate Alcoholic Drinker",
                     term == "smoke_cigs_Former" ~ "Former Cigarette Smoker",
                     term == "(Intercept)" ~ "Model Intercept")
  ) %>% 
  rename(Variables = term, "Model Coefficients" = estimate)
```

Obtaining the coefficients above shows an interesting finding. 


